{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/florian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-7.0695e-01,  2.2960e-02,  1.6947e-01, -6.0236e-01, -2.8968e-02,\n",
       "         3.3720e-01,  1.9686e-01,  8.1760e-01,  1.6311e-01, -3.6833e-01,\n",
       "         5.5707e-03,  7.7598e-02,  2.2679e-01,  1.0565e-01, -2.3633e-02,\n",
       "         4.5737e-01,  2.5490e-01,  8.4916e-02, -2.3445e-01, -3.2580e-01,\n",
       "         9.2524e-01,  1.7235e-01, -5.1883e-01,  5.2436e-02,  4.1851e-01,\n",
       "        -1.3476e-02,  1.5213e-01, -3.7569e-01, -5.8883e-01,  1.3974e-01,\n",
       "         3.7349e-01,  2.4171e-01, -5.7730e-02,  9.3303e-02,  5.5190e-02,\n",
       "        -4.8736e-01,  8.0719e-01, -5.8010e-01, -3.8817e-01,  4.6544e-01,\n",
       "        -6.6057e-01,  5.8922e-02,  3.5083e-01, -6.4317e-01, -2.7222e-01,\n",
       "        -3.6182e-01,  2.7176e-01, -6.2038e-01, -1.3663e-01, -2.8860e-01,\n",
       "        -1.2310e+00, -1.8025e-02, -1.0962e-01, -7.4949e-02, -2.9828e-01,\n",
       "         2.0283e-01, -4.9564e-02, -3.4626e-01, -4.4542e-01, -2.6465e-02,\n",
       "         3.6474e-02, -6.7238e-01,  3.2589e-02, -3.2047e-03,  2.7904e-01,\n",
       "        -4.3119e-01,  7.2844e-01,  6.0596e-01, -1.4135e+00,  4.6774e-01,\n",
       "        -2.8437e-01, -3.9221e-01,  3.8364e-01,  2.6506e-01,  2.6095e-01,\n",
       "        -2.5007e-02, -1.5903e-01, -2.5566e-01, -2.3934e-02, -3.5816e-01,\n",
       "        -4.9524e-01,  5.8411e-01,  1.3312e-01, -2.9208e-01,  1.5549e-01,\n",
       "        -2.4553e-01,  2.6783e-02,  8.3025e-02, -4.5393e-01,  5.6915e-01,\n",
       "        -1.1158e-01, -3.8998e-01, -5.4915e-01, -5.9170e-01,  3.0093e-01,\n",
       "        -3.3129e-01,  7.5755e-02, -1.8635e-01,  1.4057e-01,  1.6155e-01,\n",
       "         9.6664e-02, -8.5033e-01,  4.5685e-01, -4.4202e-01, -1.0974e-01,\n",
       "         6.3322e-02, -2.1587e-01,  4.3113e-01,  1.1359e-02,  1.1639e-01,\n",
       "         3.6726e-01,  1.5727e-01, -4.0573e-02, -1.5359e-01, -1.3265e-01,\n",
       "         1.5409e-01, -9.3013e-01,  1.4444e-01,  2.7057e-01,  3.5427e-01,\n",
       "        -3.7325e-01, -2.2174e-01,  6.9442e-01,  2.6056e-01, -1.7800e-03,\n",
       "         5.2050e-01, -3.7419e-01,  2.7597e-01, -8.5254e-02, -2.1270e-01,\n",
       "         2.3835e-01,  8.6208e-01,  8.9351e-01,  1.7749e-01, -4.5531e-02,\n",
       "         6.7499e-02,  9.9532e-02, -4.2624e-01, -6.1216e-01, -1.8020e-01,\n",
       "        -2.4578e-02,  3.1325e-01,  7.4377e-01, -4.1998e-01,  4.3851e-01,\n",
       "        -1.9591e-01, -3.4862e-01, -1.3314e-01,  4.4736e-01,  8.0933e-02,\n",
       "         1.9477e-01,  8.1213e-01, -1.6661e-02, -1.4923e-01, -1.8795e-01,\n",
       "        -4.8858e-01, -3.7285e-01,  3.8945e-01, -1.3323e-02,  3.5304e-01,\n",
       "         7.3370e-01,  3.8251e-01,  1.1771e-02,  2.6158e-01,  2.8550e-01,\n",
       "         3.9242e-01, -5.1385e-01,  4.1554e-01,  1.8304e-03, -2.9530e-01,\n",
       "         1.1425e-01, -7.6153e-01,  4.4049e-01, -5.9366e-01, -5.0658e-01,\n",
       "        -2.2159e-01,  9.3336e-02, -4.4505e-01,  3.3482e-01, -1.1833e-01,\n",
       "        -7.5001e-01,  8.3724e-01, -7.6231e-01, -8.4980e-01,  2.6954e-01,\n",
       "         1.3468e-01,  3.5217e-01, -2.1932e-01, -1.1369e-02,  2.4070e-01,\n",
       "        -5.2041e-02, -5.7600e-02,  5.0563e-02, -7.7780e-01,  4.1046e-03,\n",
       "         4.5517e-02, -7.6606e-02, -3.8875e-01, -1.2029e-01, -4.6367e-01,\n",
       "         4.6117e-01,  4.7709e-01,  2.1833e-01, -3.8062e-02,  4.7117e-01,\n",
       "         5.2094e-01, -1.0827e-01,  2.2952e-01, -1.8095e-01,  5.8729e-01,\n",
       "        -1.5543e-01,  1.6740e+00,  6.5067e-02, -1.8154e-01,  3.5455e-01,\n",
       "         2.2907e-02, -9.3841e-02, -1.9229e-01,  1.7286e-01,  5.8460e-02,\n",
       "        -6.2280e-02, -8.5477e-01, -1.8725e-01,  6.9024e-01, -2.0362e-01,\n",
       "         1.3144e+00, -1.4710e-02,  1.2411e-02,  5.2248e-01,  8.7746e-02,\n",
       "        -2.3400e-01,  1.6059e-01,  2.0838e-01, -3.0104e-01, -1.1629e-02,\n",
       "         2.9917e-01, -4.7719e-01,  2.0580e-01, -4.2463e-01, -1.6335e-01,\n",
       "        -3.6646e-03,  2.5678e-02,  1.1348e-01, -3.9485e-02, -2.1838e-01,\n",
       "         2.0960e-01,  1.5344e-01, -6.9742e-01, -7.3318e-01, -5.3543e-02,\n",
       "        -4.2090e-01, -5.7636e-01,  2.3824e-02, -2.1594e-01, -5.2728e-01,\n",
       "        -4.7415e-02, -1.8136e-01, -2.4548e-02,  3.8726e-01, -4.0238e-01,\n",
       "        -5.7437e-02,  1.3974e-01, -4.0273e-01,  5.5026e-01, -2.1976e-01,\n",
       "         4.7848e-01,  1.8745e-01,  2.7746e-01,  3.5273e-01,  3.3717e-01,\n",
       "        -4.8570e-01, -7.5269e-01, -4.3271e-01,  7.2913e-01, -6.8824e-01,\n",
       "        -5.7495e-02,  1.3367e-01,  3.1510e-02, -6.8321e-01, -1.3851e-01,\n",
       "         8.8637e-02,  4.1239e-01, -3.2703e-01,  1.2281e-01,  2.5542e-01,\n",
       "        -7.7290e-01,  2.8426e-01, -1.6354e-01,  3.9780e-03, -2.7992e-01,\n",
       "        -7.9901e-02,  3.0750e-01, -7.5518e-02, -1.3646e+00,  5.6970e-01,\n",
       "         3.8769e-01,  2.2962e-01,  1.2083e-01, -7.4366e-01, -4.6710e-01,\n",
       "         6.4277e-02, -5.5154e-01, -2.1077e-01, -6.3447e-01, -3.7040e-01,\n",
       "         1.0357e-01,  2.1473e-02,  2.3997e-01, -1.0913e+01, -3.4536e-02,\n",
       "        -2.9982e-01,  1.0223e-01,  5.8954e-02,  5.6855e-02, -1.5488e-01,\n",
       "        -2.0920e-01, -1.9575e-01, -3.1763e-01, -2.1003e-01, -8.3082e-01,\n",
       "        -9.2829e-02,  2.1493e-01,  5.2447e-01, -5.9099e-01, -2.2462e-01,\n",
       "         5.2953e-01,  3.4992e-01,  1.1516e+00, -5.6258e-01,  4.1239e-03,\n",
       "         5.6286e-01, -5.5748e-01, -4.4725e-01,  3.1215e-01, -4.8826e-01,\n",
       "        -2.3465e-01, -1.9930e-01, -8.4368e-01, -1.6110e-01, -2.4440e-01,\n",
       "        -1.1413e-01,  2.0414e-01, -6.6142e-01,  2.8557e-01, -1.3592e-01,\n",
       "        -4.6147e-01,  6.8359e-01, -7.4421e-02,  7.7393e-02, -3.1086e-01,\n",
       "        -9.7798e-02,  1.3743e-01,  7.2493e-01, -2.8112e-01,  1.3782e-01,\n",
       "         1.2593e-01,  3.0072e-01,  6.6954e-01, -1.9452e-01, -1.9412e-01,\n",
       "         1.3974e-01, -2.1588e-01,  5.9889e-01,  2.9819e-01,  4.0487e-01,\n",
       "         1.8876e-01,  6.4985e-01, -8.1193e-01,  1.0227e-02, -8.5957e-01,\n",
       "        -6.5838e-01,  3.9005e-01,  5.4679e-01, -5.0794e-01, -4.4363e-01,\n",
       "         4.5978e-01, -3.9070e-01, -1.3077e-01, -3.9185e-01,  1.3803e-01,\n",
       "         2.4640e-01, -9.8800e-01, -4.0035e-01, -2.5978e-01,  1.2195e-01,\n",
       "        -4.6805e-01,  3.5834e-01,  2.3420e-01, -5.7497e-01, -6.8026e-01,\n",
       "        -2.6641e-01, -1.5477e-01,  1.8646e-01,  2.0414e-01, -4.2078e-01,\n",
       "        -2.9283e-01, -9.2841e-01, -2.1818e-01, -4.1892e-01,  5.8247e-01,\n",
       "        -3.3253e-01,  4.1855e-01, -2.3661e-01,  3.4723e-01,  9.7326e-01,\n",
       "        -5.7639e-01, -1.4374e-01, -2.5184e-01, -2.1495e-01, -3.9188e-01,\n",
       "         1.0256e-01,  3.6822e-01,  3.8761e-01,  3.6777e-01, -6.5283e-01,\n",
       "         7.8950e-02, -9.1233e-02,  2.3547e-01,  4.9198e-01, -4.8356e-01,\n",
       "         2.6967e-01,  6.7616e-01, -3.4673e-01,  1.4821e-03, -4.3240e-01,\n",
       "        -2.4293e-01,  2.6163e-01, -9.8011e-03, -5.0962e-01,  8.4447e-02,\n",
       "        -5.1097e-01, -3.1410e-01, -7.1530e-01, -1.3886e-01, -1.8897e-01,\n",
       "         1.0254e-02, -6.6605e-01,  1.3457e-01, -2.5920e-01, -7.5678e-03,\n",
       "        -2.6790e-01,  2.7676e-02,  3.7266e-03, -7.8525e-02,  8.5686e-02,\n",
       "         1.9112e-01,  3.2993e-01, -1.0539e-01,  3.6396e-01,  8.5969e-01,\n",
       "         8.3753e-02, -2.2822e-01,  9.7563e-02,  1.0214e+00,  5.4621e-01,\n",
       "        -3.7178e-01, -2.7630e-01,  4.0372e-01, -1.3944e-01, -4.8569e-01,\n",
       "        -1.4159e-01, -2.5322e-01,  5.7300e-02, -1.1495e-01,  2.9803e-01,\n",
       "         7.0513e-03, -1.2148e-01, -1.0304e-01,  7.5605e-01,  6.5760e-02,\n",
       "        -1.3417e-02, -1.9167e-01,  1.8709e-01, -9.9453e-02, -7.5252e-01,\n",
       "         4.5520e-01,  1.1890e-02,  5.7407e-02, -2.3593e-01, -1.2277e-02,\n",
       "         2.9555e-01,  1.2320e-01,  5.0103e-01, -5.0045e-01,  1.0438e-01,\n",
       "        -2.3990e-02,  2.6226e-01,  4.0948e-01,  1.5035e-01,  6.2946e-01,\n",
       "         4.0716e-01, -1.3702e-01,  2.6403e-01,  2.4451e-01,  1.6452e-01,\n",
       "        -1.4719e-01,  8.8861e-02,  1.9537e-01,  1.0203e-01, -2.2126e-01,\n",
       "         4.3038e-02, -6.6377e-01, -7.0416e-02, -4.0151e-01,  1.8514e-01,\n",
       "         2.9852e-03,  7.6041e-02, -3.3861e-01,  3.0563e-01,  3.1054e-01,\n",
       "        -1.4636e-01, -1.6386e-01, -2.2971e-02, -2.8230e-01, -1.8472e-01,\n",
       "         5.7860e-01, -9.2104e-02,  1.7883e-02,  5.4419e-01, -2.0191e-01,\n",
       "        -2.0114e-01, -1.0003e+00, -5.3003e-01, -4.4944e-01, -5.5695e-03,\n",
       "         1.7166e-01, -7.1743e-01, -2.4827e-01, -2.9699e-01, -6.6519e-01,\n",
       "         3.1675e-04, -3.1027e-01,  5.1345e-02, -2.3266e-01, -1.5158e-01,\n",
       "        -4.1751e-01,  6.4565e-01, -4.0295e-01, -3.1046e-01, -5.9294e-01,\n",
       "         7.6879e-02,  1.5975e-01, -7.0007e-03, -8.1540e-01,  1.5877e-01,\n",
       "        -1.8668e-01, -2.3044e-01, -9.8931e-02,  2.8271e-01, -1.0345e+00,\n",
       "        -1.6702e-01, -6.7877e-01, -3.4450e-01, -1.8112e-01, -1.7646e-01,\n",
       "         1.4755e-01,  4.8611e-01,  1.0636e+00,  3.3005e-01, -1.2607e-01,\n",
       "        -5.6388e-02, -1.7470e-01,  6.1098e-01, -6.5105e-01, -1.4182e-01,\n",
       "         2.8917e-01, -1.5377e-01,  4.1987e-01,  2.0558e-01, -2.7865e-02,\n",
       "         9.9470e-02,  1.0277e+00, -1.6188e-01,  1.7338e-01,  1.3545e-01,\n",
       "         6.4382e-01,  4.4925e-01,  2.0124e-01, -2.1555e-01, -1.9694e-01,\n",
       "        -8.6006e-02,  1.5866e-01,  2.4482e-01,  1.7819e-01, -3.2255e-01,\n",
       "         1.8252e-01, -9.3950e-02, -1.5520e-01, -1.0637e-01,  1.4238e-01,\n",
       "        -3.4360e-01,  1.9829e-01, -1.8443e-01, -9.0652e-02, -7.3548e-01,\n",
       "        -9.1107e-03, -1.5049e-02,  7.8059e-03, -3.9732e-01, -2.6509e-01,\n",
       "        -1.0606e-01,  1.4195e-01,  4.7776e-01,  6.3424e-01,  8.0429e-01,\n",
       "        -8.1861e-02, -1.5779e-01, -7.3504e-01,  2.1382e-01,  6.9267e-02,\n",
       "        -5.9533e-01,  5.4931e-02, -2.6550e-01,  2.1344e-01,  4.3745e-01,\n",
       "         1.1315e-02,  7.7458e-01,  4.8353e-01,  1.5203e-01, -1.6544e-01,\n",
       "         7.6262e-01,  2.2383e-01,  2.5370e-01, -3.9140e-01, -1.2015e-02,\n",
       "        -5.4360e-01, -8.1816e-02,  1.9065e-01, -4.2959e-01,  4.7688e-02,\n",
       "         8.3173e-01,  1.7680e-01, -4.9344e-01,  3.4269e-01, -4.6646e-01,\n",
       "        -1.4977e-01, -6.7166e-01,  2.1192e-02, -1.8150e-01, -7.8198e-02,\n",
       "         1.0848e-01,  7.5747e-01, -4.8878e-01, -2.6286e-02, -2.1327e-02,\n",
       "        -7.8146e-02,  3.8887e-01,  1.8417e-01, -4.7693e-02,  7.2839e-01,\n",
       "        -2.8339e-01, -8.8172e-02, -3.8532e-01, -3.8371e-01, -4.9510e-02,\n",
       "        -1.5840e-01, -3.7090e-02,  5.3403e-01,  2.4365e-01,  2.7671e-01,\n",
       "         3.3427e-01, -4.3480e-01,  1.7429e-02,  2.1585e-01,  7.0310e-02,\n",
       "         3.4973e-01,  3.4142e-01,  3.3702e-01,  1.7445e-01,  3.1597e-01,\n",
       "         3.3290e-01, -1.0281e-01, -2.1569e-01,  1.3268e-01,  4.2859e-01,\n",
       "         3.0415e-01, -2.0277e-01, -2.7139e-01,  1.0115e-01,  3.1251e-01,\n",
       "        -1.3474e-01, -1.3764e-01,  2.2251e-02,  5.9400e-01, -1.2755e-01,\n",
       "        -1.9921e-01, -5.2271e-01, -1.8401e-01,  2.5581e-01,  9.1149e-02,\n",
       "        -1.8149e-01, -1.0858e-01,  1.6842e-01, -8.5102e-01,  4.6840e-02,\n",
       "        -2.1286e-01,  1.6714e-01,  2.1820e-01, -5.4224e-01, -2.2808e-01,\n",
       "        -1.5706e-01, -4.9843e-01,  6.6377e-01,  5.2448e-02, -1.2813e-01,\n",
       "         5.6396e-01,  5.0945e-01, -4.9020e-01,  2.3489e-01,  1.0022e-02,\n",
       "         1.5820e-01, -9.3417e-01, -1.7726e-01,  1.4509e-01, -3.5115e-01,\n",
       "         4.3466e-01, -1.4675e-01,  2.6602e-03,  7.8153e-02,  5.0083e-01,\n",
       "         2.5278e-02, -1.6908e-02, -1.4727e-01, -6.3626e-03, -3.1400e-01,\n",
       "        -4.3268e-01,  1.9439e-02, -5.8304e-03, -5.7075e-01, -4.0648e-01,\n",
       "        -4.0179e-01,  3.8209e-01, -3.9265e-01, -5.3554e-01, -8.7067e-02,\n",
       "         1.7919e-01, -3.5373e-01, -3.2915e-01,  2.8113e-01,  4.6201e-01,\n",
       "         3.3771e-01,  3.4746e-01,  1.8276e-02, -6.0035e-01, -4.5080e-01,\n",
       "        -1.8113e-01,  6.0183e-01,  6.2212e-02,  3.2942e-02,  1.0269e+00,\n",
       "        -1.6067e-01,  2.0684e-01, -2.0413e-01,  5.9426e-01,  1.0531e-01,\n",
       "        -1.1922e-01,  1.1881e-01, -6.2230e-02, -5.9091e-02, -1.5350e-01,\n",
       "        -6.2751e-01, -2.2399e-01, -7.2043e-01,  6.1189e-01,  5.9651e-01,\n",
       "        -1.6315e-01,  4.5371e-01, -8.7060e-02])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text1 = \"Electric vehicles are becoming increasingly popular. They help reduce greenhouse gas emissions and air pollution. Many governments offer incentives to promote the adoption of electric cars. Charging infrastructure is rapidly expanding in urban areas. The future of transportation seems to be electric.\"\n",
    "text2 = \"Renewable energy sources are gaining traction worldwide. Solar and wind power are becoming more cost-effective and efficient. Governments are implementing policies to encourage the use of clean energy. Innovations in energy storage, such as advanced batteries, facilitate the adoption of renewables. The shift towards sustainable energy is gaining momentum.\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def insert_sep_token(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    text_with_sep = ' [SEP] '.join(sentences)\n",
    "    return text_with_sep\n",
    "\n",
    "def bert_embed_text(text):\n",
    "    marked_text = \"[CLS] \" + insert_sep_token(text)\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create segment ids\n",
    "    segments_ids = []\n",
    "    current_segment_id = 0\n",
    "    for value in tokenized_text:\n",
    "        segments_ids.append(current_segment_id)\n",
    "        if value == \"[SEP]\":\n",
    "            current_segment_id = 1 - current_segment_id\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding2 = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text1 = \"Electric vehicles are becoming increasingly popular. They help reduce greenhouse gas emissions and air pollution. Many governments offer incentives to promote the adoption of electric cars. Charging infrastructure is rapidly expanding in urban areas. The future of transportation seems to be electric.\"\n",
    "example_text2 = \"Renewable energy sources are gaining traction worldwide. Solar and wind power are becoming more cost-effective and efficient. Governments are implementing policies to encourage the use of clean energy. Innovations in energy storage, such as advanced batteries, facilitate the adoption of renewables. The shift towards sustainable energy is gaining momentum.\"\n",
    "example_texts = [example_text1, example_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/florian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-693a48a2076d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Extract the embeddings for Text 1 and Text 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtext1_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'. '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtext2_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'. '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text1' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "def insert_sep_token(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    text_with_sep = ' [SEP] '.join(sentences)\n",
    "    return text_with_sep\n",
    "\n",
    "processed_texts = []\n",
    "# Preprocessing\n",
    "for text in example_texts:\n",
    "    processed_text = insert_sep_token(text)\n",
    "    processed_text = \"[CLS] \" + processed_text\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "inputs = tokenizer(processed_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "hidden_states = outputs.hidden_states\n",
    "last_hidden_state = hidden_states[-1]  # shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Calculate the mean of the last hidden layer for each sentence\n",
    "embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Extract the embeddings for Text 1 and Text 2\n",
    "text1_embedding = embeddings[:len(text1.split('. ')), :]\n",
    "text2_embedding = embeddings[len(text1.split('. ')):, :]\n",
    "\n",
    "# Calculate the mean embeddings for Text 1 and Text 2\n",
    "text1_mean_embedding = text1_embedding.mean(dim=0)\n",
    "text2_mean_embedding = text2_embedding.mean(dim=0)\n",
    "\n",
    "# Calculate the cosine similarity between the mean embeddings of Text 1 and Text 2\n",
    "cosine_sim = cosine_similarity(text1_mean_embedding.unsqueeze(0), text2_mean_embedding.unsqueeze(0))\n",
    "\n",
    "print(f\"Cosine similarity between Text 1 and Text 2: {cosine_sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
