{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of this file:\n",
    "Comparison between several options to create embeddings of articles:\n",
    "- Source (entire article or summary created by BART)\n",
    "- Embedding (finaly layer, second last layer, sum/concat of last 4 layers)\n",
    "\n",
    "Method: 3 articles were defined 1 base case, 1 that is similar to it and another that is different\n",
    "\n",
    "Metric: percentage difference between the similarity of similar and different summaries (PD)\n",
    "\n",
    "Results: \n",
    "- When comparing summaries PD was always around 2.5% except for comparison between final layer embeddings (PD: 7.29%)\n",
    "- When comparing the first 512 tokens of the entire article, the same pattern emerged: PD around 4-5% for all embedding options, except for the embedding extracted from the final layer, where PD went above 10%.\n",
    "\n",
    "Conclusion:\n",
    "<b> Clustering final layer embeddings of entire articles. </b>\n",
    "\n",
    "Question set 1:\n",
    "- Are the PDs that we get large enough? Is there an objective way to define a sufficient PD?\n",
    "\n",
    "\n",
    "Question set 2: (Further steps - how to use/describe clusters)\n",
    "- Choose number of clusters\n",
    "- NER\n",
    "- Summary of centroid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy import spatial\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "with open('../data/misc/article_summary_list.txt', 'r') as file:\n",
    "    summaries = [line.rstrip() for line in file]\n",
    "articles = pd.read_csv('../data/clean/\"quantumcomputing\"AND\"research\"_999.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('IBM and Google are giving $150M to two universities in the U.S. and Japan '\n",
      " 'for quantum computing research. The aim is to create a quantum supercomputer '\n",
      " 'in a decade that has 100,000 qubits. A signing ceremony is set to occur in '\n",
      " 'Hiroshima, Japan this weekend at the G-7 meetings.')\n"
     ]
    }
   ],
   "source": [
    "pprint(summaries[0]) # baseline summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The University of Chicago and the University of Tokyo are getting $150 '\n",
      " 'million for quantum computing research. Former Chicago Mayor Rahm Emanuel, '\n",
      " 'the current U.S. ambassador to Japan, said the schools’ partnership resulted '\n",
      " 'from a lunch last summer. IBM is giving the two schools $100 million while '\n",
      " 'Google is donating $50 million, according to the report.')\n"
     ]
    }
   ],
   "source": [
    "pprint(summaries[1]) # similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nikolaos “Nikos” Bogonikolos, 59, faces charges tied to wire fraud and '\n",
      " 'smuggling. He was arrested in Paris last week and faces extradition to the '\n",
      " 'U.S., the Justice Department says. The case portrays him as a man with '\n",
      " 'access to sophisticated military technology.')\n"
     ]
    }
   ],
   "source": [
    "pprint(summaries[8]) # different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nikola',\n",
       " '##os',\n",
       " '“',\n",
       " 'nik',\n",
       " '##os',\n",
       " '”',\n",
       " 'bog',\n",
       " '##oni',\n",
       " '##ko',\n",
       " '##los',\n",
       " ',',\n",
       " '59',\n",
       " ',',\n",
       " 'faces',\n",
       " 'charges',\n",
       " 'tied',\n",
       " 'to',\n",
       " 'wire',\n",
       " 'fraud',\n",
       " 'and',\n",
       " 'smuggling',\n",
       " '.',\n",
       " 'he',\n",
       " 'was',\n",
       " 'arrested',\n",
       " 'in',\n",
       " 'paris',\n",
       " 'last',\n",
       " 'week',\n",
       " 'and',\n",
       " 'faces',\n",
       " 'extra',\n",
       " '##dition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " ',',\n",
       " 'the',\n",
       " 'justice',\n",
       " 'department',\n",
       " 'says',\n",
       " '.',\n",
       " 'the',\n",
       " 'case',\n",
       " 'portrays',\n",
       " 'him',\n",
       " 'as',\n",
       " 'a',\n",
       " 'man',\n",
       " 'with',\n",
       " 'access',\n",
       " 'to',\n",
       " 'sophisticated',\n",
       " 'military',\n",
       " 'technology',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(summaries[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def insert_sep_token(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    text_with_sep = ' [SEP] '.join(sentences)\n",
    "    return text_with_sep\n",
    "\n",
    "\n",
    "def bert_embed_text(text):\n",
    "    marked_text = \"[CLS] \" + insert_sep_token(text)\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    if len(tokenized_text) > 512:\n",
    "        tokenized_text = tokenized_text[:512]\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create segment ids (alternating between 0 and 1)\n",
    "    segments_ids = []\n",
    "    current_segment_id = 0\n",
    "    for value in tokenized_text:\n",
    "        segments_ids.append(current_segment_id)\n",
    "        if value == \"[SEP]\":\n",
    "            current_segment_id = 1 - current_segment_id\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True) # output all hidden states\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # hidden states from all layers because we set output_hidden_states = True\n",
    "        # See the documentation for more details: # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # token_vecs = hidden_states[-2][0] # second to last layer\n",
    "    # sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = summaries[0]\n",
    "text2 = summaries[1]\n",
    "text3 = summaries[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = articles.at[1, 'body']\n",
    "text2 = articles.at[2, 'body']\n",
    "text3 = articles.at[9, 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Computing the hidden state for each text\n",
    "hidden_states1 = bert_embed_text(text1)\n",
    "hidden_states2 = bert_embed_text(text2)\n",
    "hidden_states3 = bert_embed_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2:  0.9642568826675415\n",
      "Percentage difference to similarity between text 1 and 3:  4.34 %\n"
     ]
    }
   ],
   "source": [
    "# Comparing second to last layer of the three texts\n",
    "token_vecs1 = hidden_states1[-2][0]\n",
    "embed1 = torch.mean(token_vecs1, dim=0)\n",
    "token_vecs2 = hidden_states2[-2][0]\n",
    "embed2 = torch.mean(token_vecs2, dim=0)\n",
    "token_vecs3 = hidden_states3[-2][0]\n",
    "embed3 = torch.mean(token_vecs3, dim=0)\n",
    "\n",
    "more_similar = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "less_similar = 1 - spatial.distance.cosine(embed1, embed3)\n",
    "\n",
    "print(\"Similarity between text1 and text2: \", more_similar)\n",
    "print(\"Percentage difference to similarity between text 1 and 3: \", round((more_similar-less_similar)/more_similar*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2:  0.94\n",
      "Percentage difference to similarity between text 1 and 3:  10.24 %\n"
     ]
    }
   ],
   "source": [
    "# Comparing last layer of the three texts\n",
    "token_vecs1 = hidden_states1[-1][0]\n",
    "embed1 = torch.mean(token_vecs1, dim=0)\n",
    "token_vecs2 = hidden_states2[-1][0]\n",
    "embed2 = torch.mean(token_vecs2, dim=0)\n",
    "token_vecs3 = hidden_states3[-1][0]\n",
    "embed3 = torch.mean(token_vecs3, dim=0)\n",
    "\n",
    "more_similar = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "less_similar = 1 - spatial.distance.cosine(embed1, embed3)\n",
    "\n",
    "print(\"Similarity between text1 and text2: \", round(more_similar, 2))\n",
    "print(\"Percentage difference to similarity between text 1 and 3: \", round((more_similar-less_similar)/more_similar*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8410043716430664"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2:  0.9580944776535034\n",
      "Percentage difference to similarity between text 1 and 3:  4.95 %\n"
     ]
    }
   ],
   "source": [
    "# Comparing sum of last four hidden layers of the three texts\n",
    "token_vecs1 = torch.sum(torch.stack(hidden_states1[-4:], dim=0), dim=0)[0]\n",
    "embed1 = torch.mean(token_vecs1, dim=0)\n",
    "token_vecs2 = torch.sum(torch.stack(hidden_states2[-4:], dim=0), dim=0)[0]\n",
    "embed2 = torch.mean(token_vecs2, dim=0)\n",
    "token_vecs3 = torch.sum(torch.stack(hidden_states3[-4:], dim=0), dim=0)[0]\n",
    "embed3 = torch.mean(token_vecs3, dim=0)\n",
    "\n",
    "more_similar = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "less_similar = 1 - spatial.distance.cosine(embed1, embed3)\n",
    "\n",
    "print(\"Similarity between text1 and text2: \", more_similar)\n",
    "print(\"Percentage difference to similarity between text 1 and 3: \", round((more_similar-less_similar)/more_similar*100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2:  0.9562898874282837\n",
      "Percentage difference to similarity between text 1 and 3:  5.05 %\n"
     ]
    }
   ],
   "source": [
    "# comparing the concatenation of the last four hidden layers of the three texts\n",
    "token_vecs1 = torch.cat(hidden_states1[-4:], dim=2)[0]\n",
    "embed1 = torch.mean(token_vecs1, dim=0)\n",
    "token_vecs2 = torch.cat(hidden_states2[-4:], dim=2)[0]\n",
    "embed2 = torch.mean(token_vecs2, dim=0)\n",
    "token_vecs3 = torch.cat(hidden_states3[-4:], dim=2)[0]\n",
    "embed3 = torch.mean(token_vecs3, dim=0)\n",
    "\n",
    "more_similar = 1 - spatial.distance.cosine(embed1, embed2)\n",
    "less_similar = 1 - spatial.distance.cosine(embed1, embed3)\n",
    "\n",
    "print(\"Similarity between text1 and text2: \", more_similar)\n",
    "print(\"Percentage difference to similarity between text 1 and 3: \", round((more_similar-less_similar)/more_similar*100, 2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
