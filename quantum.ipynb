{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting news article info:   0%|          | 0/484 [00:00<?, ?it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:   3%|▎         | 15/484 [00:02<01:04,  7.26it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:  11%|█▏        | 55/484 [00:07<00:38, 11.27it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:  14%|█▍        | 67/484 [00:10<01:16,  5.48it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:  24%|██▍       | 117/484 [00:17<00:38,  9.54it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:  81%|████████  | 392/484 [00:52<00:11,  8.11it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info:  88%|████████▊ | 426/484 [00:58<00:08,  7.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "encoding error : input conversion failed due to input error, bytes 0x44 0x00 0x00 0x00\n",
      "I/O error : encoder error\n",
      "Getting news article info: 100%|██████████| 484/484 [01:11<00:00,  6.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_index</th>\n",
       "      <th>engine</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>https://www.nytimes.com/2023/06/14/science/ibm...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Quantum Computing Advance Begins New Era, IBM ...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>https://www.nytimes.com/2023/06/14/science/ibm...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Quantum Computing Advance Begins New Era, IBM ...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>But with their intrinsic ability to consider m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>https://www.nytimes.com/2023/06/14/science/ibm...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Quantum Computing Advance Begins New Era, IBM ...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>“What IBM showed here is really an amazingly i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>https://www.nytimes.com/2023/06/14/science/ibm...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Quantum Computing Advance Begins New Era, IBM ...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>While researchers at Google in 2019 claimed th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>https://www.nytimes.com/2023/06/14/science/ibm...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Quantum Computing Advance Begins New Era, IBM ...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Quantum computers today are small in computati...</td>\n",
       "      <td>Present-day computers are called digital, or c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_index  engine                                               link  \\\n",
       "25              1  Google  https://www.nytimes.com/2023/06/14/science/ibm...   \n",
       "26              1  Google  https://www.nytimes.com/2023/06/14/science/ibm...   \n",
       "27              1  Google  https://www.nytimes.com/2023/06/14/science/ibm...   \n",
       "28              1  Google  https://www.nytimes.com/2023/06/14/science/ibm...   \n",
       "31              1  Google  https://www.nytimes.com/2023/06/14/science/ibm...   \n",
       "\n",
       "                source                                              title  \\\n",
       "25  The New York Times  Quantum Computing Advance Begins New Era, IBM ...   \n",
       "26  The New York Times  Quantum Computing Advance Begins New Era, IBM ...   \n",
       "27  The New York Times  Quantum Computing Advance Begins New Era, IBM ...   \n",
       "28  The New York Times  Quantum Computing Advance Begins New Era, IBM ...   \n",
       "31  The New York Times  Quantum Computing Advance Begins New Era, IBM ...   \n",
       "\n",
       "                                          description  \\\n",
       "25  Quantum computers today are small in computati...   \n",
       "26  Quantum computers today are small in computati...   \n",
       "27  Quantum computers today are small in computati...   \n",
       "28  Quantum computers today are small in computati...   \n",
       "31  Quantum computers today are small in computati...   \n",
       "\n",
       "                                                 body  \\\n",
       "25  Quantum computers today are small in computati...   \n",
       "26  Quantum computers today are small in computati...   \n",
       "27  Quantum computers today are small in computati...   \n",
       "28  Quantum computers today are small in computati...   \n",
       "31  Quantum computers today are small in computati...   \n",
       "\n",
       "                                            paragraph  \n",
       "25  Quantum computers today are small in computati...  \n",
       "26  But with their intrinsic ability to consider m...  \n",
       "27  “What IBM showed here is really an amazingly i...  \n",
       "28  While researchers at Google in 2019 claimed th...  \n",
       "31  Present-day computers are called digital, or c...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline from the etl_pipeline package\n",
    "from pipeline_executor import PipelineExecutor\n",
    "from nlp_analysis.NER import extract_entities\n",
    "topic = 'Quantum Computing' # \"quantumcomputing\"AND\"research\"\n",
    "\n",
    "# create a pipeline executor\n",
    "pipeline_executor = PipelineExecutor()\n",
    "quantum_df = pipeline_executor.execute(query=topic, max_articles=999, overwrite=True)\n",
    "\n",
    "# print the first 5 rows of the dataframe\n",
    "quantum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/clean/QuantumComputing_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_analysis.word_wizard import WordWizard\n",
    "\n",
    "# create a word wizard\n",
    "word_wizard = WordWizard(df)\n",
    "word_wizard.create_word_embeddings(columns=['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_wizard.cluster_embeddings(column='body', method='silhouette')\n",
    "word_wizard.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_wizard.df['body_word_embeddings_cluster_13_is_medoid'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each entity run the pipeline (adding \"quantum\" to the query) and store the results in a dictionary\n",
    "entity_results = {}\n",
    "for entity in entities:\n",
    "    entity_results[entity] = pipeline_executor.execute(query=f\"'{entity}' AND '{topic}'\", max_articles=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results for the first entity\n",
    "entity_results['Microsoft'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO summarize the results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code (Chooses NVIDIA or Metal backend if available, otherwise defaults to CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_df = entity_results['Microsoft']\n",
    "apple_df = entity_results['Apple']\n",
    "ibm_df = entity_results['IBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_df.shape, apple_df.shape, ibm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_bart(\n",
    "    column: pd.Series, tokenizer: BartTokenizer, model: BartForConditionalGeneration, min_len=50, max_len=100) -> str:\n",
    "    \n",
    "    long_text = column.str.cat()\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(long_text)\n",
    "\n",
    "    # initialize\n",
    "    length = 0\n",
    "    chunk = \"\"\n",
    "    chunks = []\n",
    "    count = -1\n",
    "    for sentence in sentences:\n",
    "        count += 1\n",
    "        combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
    "\n",
    "        if combined_length  <= 1024: # if it doesn't exceed  -tokenizer.max_len_single_sentence-\n",
    "            chunk += sentence + \" \" # add the sentence to the chunk\n",
    "            length = combined_length # update the length counter\n",
    "\n",
    "            # if it is the last sentence\n",
    "            if count == len(sentences) - 1:\n",
    "                chunks.append(chunk) # save the chunk\n",
    "            \n",
    "        else: \n",
    "            chunks.append(chunk) # save the chunk\n",
    "            # reset \n",
    "            length = 0 \n",
    "            chunk = \"\"\n",
    "\n",
    "            # take care of the overflow sentence\n",
    "            chunk += sentence + \" \"\n",
    "            length = len(tokenizer.tokenize(sentence))\n",
    "\n",
    "    # inputs\n",
    "    inputs = [tokenizer(chunk, return_tensors=\"pt\").to(device) for chunk in chunks]\n",
    "\n",
    "    # print summary\n",
    "    outputs = []\n",
    "    for input in tqdm(inputs):\n",
    "        output = model.generate(**input, num_beams=2, min_length=min_len, max_length=max_len)\n",
    "        outputs.append(tokenizer.decode(*output, skip_special_tokens=True))\n",
    "\n",
    "    return \" \".join(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for Microsoft and Apple based on News Titles\n",
    "microsoft_titles_summary = summarize_bart(microsoft_df[\"title\"], tokenizer, model)\n",
    "apple_titles_summary = summarize_bart(apple_df[\"title\"], tokenizer, model)\n",
    "ibm_titles_summary = summarize_bart(ibm_df[\"title\"], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_titles_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_titles_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_titles_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for Microsoft and Apple based on News article bodies\n",
    "microsoft_bodies_summary = summarize_bart(microsoft_df[\"body\"], tokenizer, model)\n",
    "apple_bodies_summary = summarize_bart(apple_df[\"body\"], tokenizer, model)\n",
    "ibm_bodies_summary = summarize_bart(ibm_df[\"body\"], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the long summary\n",
    "def sub1024_summ(text: str, tokenizer: BartTokenizer, model: BartForConditionalGeneration, min_len=50, max_len=100) -> str:\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=min_len, max_length=max_len)\n",
    "    return tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1024_summ(microsoft_bodies_summary, tokenizer, model, min_len=100, max_len=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1024_summ(apple_bodies_summary, tokenizer, model, min_len=100, max_len=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1024_summ(ibm_bodies_summary, tokenizer, model, min_len=100, max_len=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngl... not the best summaries I think"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
