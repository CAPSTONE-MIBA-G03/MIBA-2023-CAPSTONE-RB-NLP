{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 18:26:02.343032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from pipeline_executor import PipelineExecutor\n",
    "import nlp_analysis.word_wizard as ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting news article info:  14%|█▍        | 81/588 [01:00<03:59,  2.12it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  19%|█▊        | 109/588 [01:22<03:33,  2.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  29%|██▊       | 168/588 [02:08<05:37,  1.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  29%|██▉       | 170/588 [02:10<06:14,  1.12it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  34%|███▍      | 202/588 [02:45<04:38,  1.38it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  35%|███▌      | 206/588 [02:46<03:03,  2.08it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  78%|███████▊  | 460/588 [05:37<00:52,  2.43it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  82%|████████▏ | 482/588 [05:52<01:24,  1.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  82%|████████▏ | 484/588 [05:53<01:09,  1.49it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  93%|█████████▎| 546/588 [06:33<00:28,  1.47it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info: 100%|██████████| 588/588 [07:01<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "topic = \"roche\"\n",
    "\n",
    "pipeline_executor = PipelineExecutor()\n",
    "roche = pipeline_executor.execute(query=topic, max_articles=None, overwrite=True)\n",
    "\n",
    "# safe and load quantum_df using parquet\n",
    "roche.to_parquet('data/clean/roche.gzip', compression='gzip')\n",
    "roche = pd.read_parquet('data/clean/roche.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraphs - Word - silhouette\n",
    "wizard = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard.create_word_embeddings(wizard.interest) \\\n",
    "    .cluster_embeddings(wizard.interest, algorithm = 'hdbscan') \\\n",
    "    .entitiy_recognition(wizard.interest) \\\n",
    "    .summarize_medoids(wizard.interest) \\\n",
    "    .find_sentiment(wizard.interest) \\\n",
    "    .topic_modelling(wizard.interest)\n",
    "\n",
    "\n",
    "# Paragraphs - Sentence - silhouette\n",
    "wizard2 = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard2.create_sentence_embeddings(wizard2.interest) \\\n",
    "    .cluster_embeddings(wizard2.interest, algorithm = 'hdbscan') \\\n",
    "    .entitiy_recognition(wizard2.interest) \\\n",
    "    .summarize_medoids(wizard2.interest) \\\n",
    "    .find_sentiment(wizard2.interest) \\\n",
    "    .topic_modelling(wizard2.interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article - Word - silhouette\n",
    "wizard3 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard3.create_word_embeddings(wizard3.interest) \\\n",
    "    .cluster_embeddings(wizard3.interest, algorithm = 'hdbscan') \\\n",
    "    .entitiy_recognition(wizard3.interest) \\\n",
    "    .summarize_medoids(wizard3.interest) \\\n",
    "    .find_sentiment(wizard3.interest) \\\n",
    "    .topic_modelling(wizard3.interest)\n",
    "\n",
    "# Article - Sentence - silhouette\n",
    "wizard4 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard4.create_sentence_embeddings(wizard4.interest) \\\n",
    "    .cluster_embeddings(wizard4.interest, algorithm = 'hdbscan') \\\n",
    "    .entitiy_recognition(wizard4.interest) \\\n",
    "    .summarize_medoids(wizard4.interest) \\\n",
    "    .find_sentiment(wizard4.interest) \\\n",
    "    .topic_modelling(wizard4.interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Creating word embeddings for paragraph: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n",
      "Extracting organizations for column paragraph: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Creating summaries for medoids of column paragraph: 100%|██████████| 6/6 [04:28<00:00, 44.71s/it]\n",
      "Calculating sentiment for column paragraph: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "Extracting organizations for column paragraph: 100%|██████████| 11/11 [00:01<00:00,  8.28it/s]\n",
      "Creating summaries for medoids of column paragraph: 100%|██████████| 15/15 [08:36<00:00, 34.45s/it]\n",
      "Calculating sentiment for column paragraph: 100%|██████████| 20/20 [00:26<00:00,  1.33s/it]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'article'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m wizard2\u001b[39m.\u001b[39mtopic_modelling(wizard2\u001b[39m.\u001b[39minterest)\n\u001b[1;32m     20\u001b[0m wizard3 \u001b[39m=\u001b[39m ww\u001b[39m.\u001b[39mWordWizard(df\u001b[39m=\u001b[39mroche, interest \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m wizard3\u001b[39m.\u001b[39;49mcreate_word_embeddings(wizard3\u001b[39m.\u001b[39;49minterest)\n\u001b[1;32m     22\u001b[0m wizard3\u001b[39m.\u001b[39mcluster_embeddings(wizard3\u001b[39m.\u001b[39minterest)\n\u001b[1;32m     23\u001b[0m wizard3\u001b[39m.\u001b[39mentitiy_recognition(wizard3\u001b[39m.\u001b[39minterest)\n",
      "File \u001b[0;32m~/Documents/Learning/MiBA/classes/term3/Capstone/MIBA-2023-CAPSTONE-RB-NLP/nlp_analysis/word_wizard.py:97\u001b[0m, in \u001b[0;36mWordWizard.create_word_embeddings\u001b[0;34m(self, column, lean, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m new_column_name \u001b[39m=\u001b[39m column \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEMB_SUFFIX \u001b[39m# e.g.: paragraph_word_embeddings\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[new_column_name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfor\u001b[39;00m i, entry \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf[column], desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreating word embeddings for \u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m     98\u001b[0m     encoded_input \u001b[39m=\u001b[39m tokenizer(entry, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m     encoded_input\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article'"
     ]
    }
   ],
   "source": [
    "# Paragraphs - Word - silhouette\n",
    "interest = 'paragraph'\n",
    "wizard5 = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard5.create_word_embeddings(wizard5.interest) \\\n",
    "    .cluster_embeddings(wizard5.interest) \\\n",
    "    .entitiy_recognition(wizard5.interest) \\\n",
    "    .summarize_medoids(wizard5.interest) \\\n",
    "    .find_sentiment(wizard5.interest) \\\n",
    "    .topic_modelling(wizard5.interest)\n",
    "\n",
    "\n",
    "# Paragraphs - Sentence - silhouette\n",
    "wizard6 = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard6.create_sentence_embeddings(wizard6.interest) \\\n",
    "    .cluster_embeddings(wizard6.interest) \\\n",
    "    .entitiy_recognition(wizard6.interest) \\\n",
    "    .summarize_medoids(wizard6.interest) \\\n",
    "    .find_sentiment(wizard6.interest) \\\n",
    "    .topic_modelling(wizard6.interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Creating word embeddings for body: 100%|██████████| 18/18 [01:27<00:00,  4.88s/it]\n",
      "Extracting organizations for column body: 100%|██████████| 2/2 [00:22<00:00, 11.08s/it]\n",
      "Creating summaries for medoids of column body:   0%|          | 0/4 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (128) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Creating summaries for medoids of column body: 100%|██████████| 4/4 [06:20<00:00, 95.01s/it] \n",
      "Calculating sentiment for column body: 100%|██████████| 18/18 [02:29<00:00,  8.28s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "Extracting organizations for column body: 100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n",
      "Creating summaries for medoids of column body:   0%|          | 0/12 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (128) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Creating summaries for medoids of column body:  17%|█▋        | 2/12 [03:24<17:03, 102.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Article - Sentence - silhouette\u001b[39;00m\n\u001b[1;32m     14\u001b[0m wizard4 \u001b[39m=\u001b[39m ww\u001b[39m.\u001b[39mWordWizard(df\u001b[39m=\u001b[39mroche, interest \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m wizard4\u001b[39m.\u001b[39;49mcreate_sentence_embeddings(wizard4\u001b[39m.\u001b[39;49minterest) \\\n\u001b[1;32m     16\u001b[0m     \u001b[39m.\u001b[39;49mcluster_embeddings(wizard4\u001b[39m.\u001b[39;49minterest) \\\n\u001b[1;32m     17\u001b[0m     \u001b[39m.\u001b[39;49mentitiy_recognition(wizard4\u001b[39m.\u001b[39;49minterest) \\\n\u001b[0;32m---> 18\u001b[0m     \u001b[39m.\u001b[39;49msummarize_medoids(wizard4\u001b[39m.\u001b[39;49minterest) \\\n\u001b[1;32m     19\u001b[0m     \u001b[39m.\u001b[39mfind_sentiment(wizard4\u001b[39m.\u001b[39minterest) \\\n\u001b[1;32m     20\u001b[0m     \u001b[39m.\u001b[39mtopic_modelling(wizard4\u001b[39m.\u001b[39minterest)\n",
      "File \u001b[0;32m~/Documents/Learning/MiBA/classes/term3/Capstone/MIBA-2023-CAPSTONE-RB-NLP/nlp_analysis/word_wizard.py:182\u001b[0m, in \u001b[0;36mWordWizard.summarize_medoids\u001b[0;34m(self, column, lean, device)\u001b[0m\n\u001b[1;32m    179\u001b[0m encoded_input\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m--> 182\u001b[0m     summary \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m    183\u001b[0m     out \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(summary[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mat[pos, new_column_name] \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1604\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1598\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1599\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1600\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1601\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1602\u001b[0m     )\n\u001b[1;32m   1603\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1605\u001b[0m         input_ids,\n\u001b[1;32m   1606\u001b[0m         beam_scorer,\n\u001b[1;32m   1607\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1608\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1609\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1610\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1611\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1612\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1613\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1614\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1615\u001b[0m     )\n\u001b[1;32m   1617\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1618\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2902\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2900\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2902\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2903\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2904\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2905\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2906\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2907\u001b[0m )\n\u001b[1;32m   2909\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2910\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/pegasus/modeling_pegasus.py:1433\u001b[0m, in \u001b[0;36mPegasusForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1413\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1416\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1417\u001b[0m     input_ids,\n\u001b[1;32m   1418\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1432\u001b[0m )\n\u001b[0;32m-> 1433\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(outputs[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1435\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Article - Word - silhouette\n",
    "wizard7 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard7.create_word_embeddings(wizard7.interest) \\\n",
    "    .cluster_embeddings(wizard7.interest) \\\n",
    "    .entitiy_recognition(wizard7.interest) \\\n",
    "    .summarize_medoids(wizard7.interest) \\\n",
    "    .find_sentiment(wizard7.interest) \\\n",
    "    .topic_modelling(wizard7.interest)\n",
    "\n",
    "# Article - Sentence - silhouette\n",
    "wizard8 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard8.create_sentence_embeddings(wizard8.interest) \\\n",
    "    .cluster_embeddings(wizard8.interest) \\\n",
    "    .entitiy_recognition(wizard8.interest) \\\n",
    "    .summarize_medoids(wizard8.interest) \\\n",
    "    .find_sentiment(wizard8.interest) \\\n",
    "    .topic_modelling(wizard8.interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18544431"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silhouette score ain't working with too few samples I think\n",
    "from sklearn.cluster import KMeans\n",
    "df = wizard.df\n",
    "from sklearn.metrics import silhouette_score\n",
    "column = 'paragraph'\n",
    "k=2\n",
    "embed_column = column + wizard.EMB_SUFFIX\n",
    "kk = KMeans(n_clusters=k, n_init='auto').fit(df[embed_column].tolist())\n",
    "labels = kk.labels_\n",
    "silhouette_score(df[embed_column].tolist(), labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8\n",
       "0    3\n",
       "4    2\n",
       "2    2\n",
       "7    1\n",
       "5    1\n",
       "6    1\n",
       "3    1\n",
       "8    1\n",
       "Name: paragraph_sentence_embeddings_clusters, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of entities in each cluster\n",
    "wizard.df['paragraph_sentence_embeddings_clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_sentence_embeddings</th>\n",
       "      <th>paragraph_sentence_embeddings_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.037035826593637466, -0.008866664953529835, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.029360659420490265, -0.06645254045724869, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.054638300091028214, -0.03934699669480324, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.021085450425744057, -0.07643812894821167, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.07643575221300125, -0.07916200160980225, -...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.047656454145908356, -0.03696030005812645, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0005669331876561046, -0.07934823632240295, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.059580232948064804, -0.022906964644789696, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-0.05660158023238182, 0.0067533645778894424, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.02275889180600643, -0.04268726333975792, -0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.024973241612315178, -0.02497449889779091, -...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.011134074069559574, -0.08654405176639557, -...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[-0.07882263511419296, -0.08047624677419662, -...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        paragraph_sentence_embeddings  \\\n",
       "0   [0.037035826593637466, -0.008866664953529835, ...   \n",
       "1   [-0.029360659420490265, -0.06645254045724869, ...   \n",
       "2   [-0.054638300091028214, -0.03934699669480324, ...   \n",
       "4   [0.021085450425744057, -0.07643812894821167, -...   \n",
       "5   [-0.07643575221300125, -0.07916200160980225, -...   \n",
       "6   [0.047656454145908356, -0.03696030005812645, -...   \n",
       "8   [0.0005669331876561046, -0.07934823632240295, ...   \n",
       "9   [0.059580232948064804, -0.022906964644789696, ...   \n",
       "11  [-0.05660158023238182, 0.0067533645778894424, ...   \n",
       "12  [0.02275889180600643, -0.04268726333975792, -0...   \n",
       "13  [0.024973241612315178, -0.02497449889779091, -...   \n",
       "14  [0.011134074069559574, -0.08654405176639557, -...   \n",
       "15  [-0.07882263511419296, -0.08047624677419662, -...   \n",
       "\n",
       "    paragraph_sentence_embeddings_clusters  \n",
       "0                                        0  \n",
       "1                                        4  \n",
       "2                                        7  \n",
       "4                                        1  \n",
       "5                                        5  \n",
       "6                                        1  \n",
       "8                                        6  \n",
       "9                                        0  \n",
       "11                                       3  \n",
       "12                                       4  \n",
       "13                                       8  \n",
       "14                                       2  \n",
       "15                                       2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of medoids per cluster\n",
    "wizard.df[wizard.df['paragraph_sentence_embeddings_clusters_medoids'] == True][['paragraph_sentence_embeddings', 'paragraph_sentence_embeddings_clusters']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
