{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-06-21 13:27:18.442421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from pipeline_executor import PipelineExecutor\n",
    "import nlp_analysis.word_wizard as ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting news article info:  14%|█▍        | 81/588 [01:00<03:59,  2.12it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  19%|█▊        | 109/588 [01:22<03:33,  2.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  29%|██▊       | 168/588 [02:08<05:37,  1.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  29%|██▉       | 170/588 [02:10<06:14,  1.12it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  34%|███▍      | 202/588 [02:45<04:38,  1.38it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  35%|███▌      | 206/588 [02:46<03:03,  2.08it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  78%|███████▊  | 460/588 [05:37<00:52,  2.43it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  82%|████████▏ | 482/588 [05:52<01:24,  1.25it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  82%|████████▏ | 484/588 [05:53<01:09,  1.49it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info:  93%|█████████▎| 546/588 [06:33<00:28,  1.47it/s]encoding error : input conversion failed due to input error, bytes 0x21 0x00 0x00 0x00\n",
      "Getting news article info: 100%|██████████| 588/588 [07:01<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "topic = \"roche\"\n",
    "\n",
    "pipeline_executor = PipelineExecutor()\n",
    "roche = pipeline_executor.execute(query=topic, max_articles=None, overwrite=True)\n",
    "\n",
    "# safe and load quantum_df using parquet\n",
    "roche.to_parquet('data/clean/roche.gzip', compression='gzip')\n",
    "roche = pd.read_parquet('data/clean/roche.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Creating word embeddings for paragraph: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n",
      "Extracting organizations for column paragraph: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Creating summaries for medoids of column paragraph: 100%|██████████| 6/6 [04:28<00:00, 44.71s/it]\n",
      "Calculating sentiment for column paragraph: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "Extracting organizations for column paragraph: 100%|██████████| 11/11 [00:01<00:00,  8.28it/s]\n",
      "Creating summaries for medoids of column paragraph: 100%|██████████| 15/15 [08:36<00:00, 34.45s/it]\n",
      "Calculating sentiment for column paragraph: 100%|██████████| 20/20 [00:26<00:00,  1.33s/it]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'article'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m wizard2\u001b[39m.\u001b[39mtopic_modelling(wizard2\u001b[39m.\u001b[39minterest)\n\u001b[1;32m     20\u001b[0m wizard3 \u001b[39m=\u001b[39m ww\u001b[39m.\u001b[39mWordWizard(df\u001b[39m=\u001b[39mroche, interest \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m wizard3\u001b[39m.\u001b[39;49mcreate_word_embeddings(wizard3\u001b[39m.\u001b[39;49minterest)\n\u001b[1;32m     22\u001b[0m wizard3\u001b[39m.\u001b[39mcluster_embeddings(wizard3\u001b[39m.\u001b[39minterest)\n\u001b[1;32m     23\u001b[0m wizard3\u001b[39m.\u001b[39mentitiy_recognition(wizard3\u001b[39m.\u001b[39minterest)\n",
      "File \u001b[0;32m~/Documents/Learning/MiBA/classes/term3/Capstone/MIBA-2023-CAPSTONE-RB-NLP/nlp_analysis/word_wizard.py:97\u001b[0m, in \u001b[0;36mWordWizard.create_word_embeddings\u001b[0;34m(self, column, lean, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m new_column_name \u001b[39m=\u001b[39m column \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEMB_SUFFIX \u001b[39m# e.g.: paragraph_word_embeddings\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[new_column_name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfor\u001b[39;00m i, entry \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf[column], desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreating word embeddings for \u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m     98\u001b[0m     encoded_input \u001b[39m=\u001b[39m tokenizer(entry, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m     encoded_input\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'article'"
     ]
    }
   ],
   "source": [
    "roche = pd.read_parquet('data/clean/roche.gzip')\n",
    "roche = roche.sample(n=20)\n",
    "importlib.reload(ww)\n",
    "\n",
    "# Paragraphs - Word - silhouette\n",
    "wizard = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard.create_word_embeddings(wizard.interest)\n",
    "wizard.cluster_embeddings(wizard.interest)\n",
    "wizard.entitiy_recognition(wizard.interest)\n",
    "wizard.summarize_medoids(wizard.interest)\n",
    "wizard.find_sentiment(wizard.interest)\n",
    "wizard.topic_modelling(wizard.interest)\n",
    "\n",
    "# Paragraphs - Sentence - silhouette\n",
    "wizard2 = ww.WordWizard(df=roche, interest = 'paragraph')\n",
    "wizard2.create_sentence_embeddings(wizard2.interest)\n",
    "wizard2.cluster_embeddings(wizard2.interest)\n",
    "wizard2.entitiy_recognition(wizard2.interest)\n",
    "wizard2.summarize_medoids(wizard2.interest)\n",
    "wizard2.find_sentiment(wizard2.interest)\n",
    "wizard2.topic_modelling(wizard2.interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Creating word embeddings for body: 100%|██████████| 18/18 [00:34<00:00,  1.94s/it]\n",
      "Extracting organizations for column body: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]\n",
      "Creating summaries for medoids of column body: 100%|██████████| 4/4 [05:14<00:00, 78.64s/it]\n",
      "Calculating sentiment for column body: 100%|██████████| 18/18 [01:31<00:00,  5.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Extracting organizations for column body: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\n",
      "Creating summaries for medoids of column body: 100%|██████████| 4/4 [04:24<00:00, 66.17s/it]\n",
      "Calculating sentiment for column body: 100%|██████████| 18/18 [01:30<00:00,  5.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [('sister', 0.0012362062942943084),\n",
       "  ('baseline', 0.0012362062942943084),\n",
       "  ('candace', 0.0012362062942943084),\n",
       "  ('forms', 0.0012362062942943084),\n",
       "  ('brain', 0.0012362062942943084),\n",
       "  ('minutes', 0.0012362062942943084),\n",
       "  ('toiletries', 0.0012362062942943084),\n",
       "  ('relapsing', 0.0012362062942943084),\n",
       "  ('did', 0.0012362062942943084),\n",
       "  ('restrictions', 0.0012362062942943084),\n",
       "  ('airports', 0.0012362062942943084),\n",
       "  ('believe', 0.0012362062942943084),\n",
       "  ('beacon', 0.0012362062942943084),\n",
       "  ('mcmichael', 0.0012362062942943084),\n",
       "  ('ongoing', 0.0012362062942943084),\n",
       "  ('tolerance', 0.0012362062942943084),\n",
       "  ('compared', 0.0012325493141377448),\n",
       "  ('wear', 0.0012325493141377448),\n",
       "  ('program', 0.0012325493141377448),\n",
       "  ('carry', 0.0012325493141377448)],\n",
       " 1: [('france', 0.0011880020236991842),\n",
       "  ('mea', 0.0011880020236991842),\n",
       "  ('scannell', 0.0011880020236991842),\n",
       "  ('dynamics', 0.0011880020236991842),\n",
       "  ('job', 0.0011880020236991842),\n",
       "  ('drugsrevenue', 0.0011880020236991842),\n",
       "  ('marketinsightsreports', 0.0011880020236991842),\n",
       "  ('industryresearch', 0.0011880020236991842),\n",
       "  ('apac', 0.0011844876427688517),\n",
       "  ('mode', 0.0011844876427688517),\n",
       "  ('microcatheters', 0.0011844876427688517),\n",
       "  ('bayer', 0.0011844876427688517),\n",
       "  ('insights', 0.0011844876427688517),\n",
       "  ('covers', 0.0011844876427688517),\n",
       "  ('cagr', 0.0011844876427688517),\n",
       "  ('italy', 0.0011844876427688517),\n",
       "  ('depth', 0.0011844876427688517),\n",
       "  ('manufacturing', 0.0011844876427688517),\n",
       "  ('hospitals', 0.0011844876427688517),\n",
       "  ('table', 0.0011844876427688517)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Article - Word - silhouette\n",
    "wizard3 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard3.create_word_embeddings(wizard3.interest)\n",
    "wizard3.cluster_embeddings(wizard3.interest)\n",
    "wizard3.entitiy_recognition(wizard3.interest)\n",
    "wizard3.summarize_medoids(wizard3.interest)\n",
    "wizard3.find_sentiment(wizard3.interest)\n",
    "wizard3.topic_modelling(wizard3.interest)\n",
    "\n",
    "# Article - Sentence - silhouette\n",
    "wizard4 = ww.WordWizard(df=roche, interest = 'body')\n",
    "wizard4.create_sentence_embeddings(wizard4.interest)\n",
    "wizard4.cluster_embeddings(wizard4.interest)\n",
    "wizard4.entitiy_recognition(wizard4.interest)\n",
    "wizard4.summarize_medoids(wizard4.interest)\n",
    "wizard4.find_sentiment(wizard4.interest)\n",
    "wizard4.topic_modelling(wizard4.interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18544431"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silhouette score ain't working with too few samples I think\n",
    "from sklearn.cluster import KMeans\n",
    "df = wizard.df\n",
    "from sklearn.metrics import silhouette_score\n",
    "column = 'paragraph'\n",
    "k=2\n",
    "embed_column = column + wizard.EMB_SUFFIX\n",
    "kk = KMeans(n_clusters=k, n_init='auto').fit(df[embed_column].tolist())\n",
    "labels = kk.labels_\n",
    "silhouette_score(df[embed_column].tolist(), labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8\n",
       "0    3\n",
       "4    2\n",
       "2    2\n",
       "7    1\n",
       "5    1\n",
       "6    1\n",
       "3    1\n",
       "8    1\n",
       "Name: paragraph_sentence_embeddings_clusters, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of entities in each cluster\n",
    "wizard.df['paragraph_sentence_embeddings_clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_sentence_embeddings</th>\n",
       "      <th>paragraph_sentence_embeddings_clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.037035826593637466, -0.008866664953529835, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.029360659420490265, -0.06645254045724869, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.054638300091028214, -0.03934699669480324, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.021085450425744057, -0.07643812894821167, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.07643575221300125, -0.07916200160980225, -...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.047656454145908356, -0.03696030005812645, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0005669331876561046, -0.07934823632240295, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.059580232948064804, -0.022906964644789696, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-0.05660158023238182, 0.0067533645778894424, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.02275889180600643, -0.04268726333975792, -0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.024973241612315178, -0.02497449889779091, -...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.011134074069559574, -0.08654405176639557, -...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[-0.07882263511419296, -0.08047624677419662, -...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        paragraph_sentence_embeddings  \\\n",
       "0   [0.037035826593637466, -0.008866664953529835, ...   \n",
       "1   [-0.029360659420490265, -0.06645254045724869, ...   \n",
       "2   [-0.054638300091028214, -0.03934699669480324, ...   \n",
       "4   [0.021085450425744057, -0.07643812894821167, -...   \n",
       "5   [-0.07643575221300125, -0.07916200160980225, -...   \n",
       "6   [0.047656454145908356, -0.03696030005812645, -...   \n",
       "8   [0.0005669331876561046, -0.07934823632240295, ...   \n",
       "9   [0.059580232948064804, -0.022906964644789696, ...   \n",
       "11  [-0.05660158023238182, 0.0067533645778894424, ...   \n",
       "12  [0.02275889180600643, -0.04268726333975792, -0...   \n",
       "13  [0.024973241612315178, -0.02497449889779091, -...   \n",
       "14  [0.011134074069559574, -0.08654405176639557, -...   \n",
       "15  [-0.07882263511419296, -0.08047624677419662, -...   \n",
       "\n",
       "    paragraph_sentence_embeddings_clusters  \n",
       "0                                        0  \n",
       "1                                        4  \n",
       "2                                        7  \n",
       "4                                        1  \n",
       "5                                        5  \n",
       "6                                        1  \n",
       "8                                        6  \n",
       "9                                        0  \n",
       "11                                       3  \n",
       "12                                       4  \n",
       "13                                       8  \n",
       "14                                       2  \n",
       "15                                       2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of medoids per cluster\n",
    "wizard.df[wizard.df['paragraph_sentence_embeddings_clusters_medoids'] == True][['paragraph_sentence_embeddings', 'paragraph_sentence_embeddings_clusters']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
